---
title: "AI Providers Overview"
description: "Configure AI providers in EnConvo"
---

## Overview

EnConvo supports a wide range of AI providers, giving you the flexibility to choose the best model for your needs. You can use cloud-based APIs or run models locally for complete privacy.

## Setup Steps

<Steps>
  <Step title="Open Settings">
    Go to **Settings** â†’ **AI Provider**
  </Step>
  <Step title="Select Provider">
    Choose your preferred LLM provider from the list
  </Step>
  <Step title="Configure Credentials">
    Go to **Credentials** module and enter your API key
  </Step>
  <Step title="Select Model">
    Choose your preferred model from the dropdown
  </Step>
</Steps>

## Supported Providers

### Cloud Providers

| Provider | Description | Link |
|----------|-------------|------|
| **EnConvo Cloud** | Built-in LLM service with points system | [docs](/providers/enconvo) |
| **OpenAI** | GPT-4o, GPT-4, o1, o3 models | [docs](/providers/openai) |
| **Anthropic** | Claude 4, Claude 3.5 Sonnet | [docs](/providers/anthropic) |
| **Google** | Gemini 2.5, Gemini 3 Pro | [docs](/providers/google) |
| **DeepSeek** | DeepSeek Chat, DeepSeek Reasoner | [docs](/providers/deepseek) |
| **Groq** | Ultra-fast inference | [docs](/providers/groq) |
| **xAI** | Grok models | [docs](/providers/xai) |
| **Mistral** | Mistral Large, Mistral Small | [docs](/providers/mistral) |
| **Qwen** | Alibaba's Qwen models | [docs](/providers/qwen) |
| **Cohere** | Command R models | [docs](/providers/cohere) |
| **Perplexity** | Sonar search models | [docs](/providers/perplexity) |

### Aggregators & Gateways

| Provider | Description | Link |
|----------|-------------|------|
| **OpenRouter** | Access 100+ models via one API | [docs](/providers/openrouter) |
| **Vercel AI Gateway** | Multi-provider gateway | [docs](/providers/vercel) |
| **Together AI** | Open-source model hosting | [docs](/providers/together) |
| **Fireworks** | Fast inference platform | [docs](/providers/fireworks) |
| **SiliconFlow** | Chinese AI platform | [docs](/providers/siliconflow) |
| **302.AI** | Multi-model platform | [docs](/providers/302ai) |

### Enterprise

| Provider | Description | Link |
|----------|-------------|------|
| **Azure OpenAI** | Microsoft's OpenAI service | [docs](/providers/azure) |
| **Cloudflare Workers AI** | Edge AI inference | [docs](/providers/cloudflare) |
| **DashScope** | Alibaba Cloud AI | [docs](/providers/dashscope) |

### Local Models

| Provider | Description | Link |
|----------|-------------|------|
| **Ollama** | Run local LLMs easily | [docs](/providers/ollama) |
| **LM Studio** | GUI for local models | [docs](/providers/lmstudio) |

### Other Providers

| Provider | Description |
|----------|-------------|
| **MoonShot AI** | Kimi models |
| **Sambanova** | High-performance inference |
| **Cerebras** | Fast inference hardware |
| **Poe** | Multi-model chat platform |
| **Straico** | AI aggregator |
| **AIMagicX** | Multi-model platform |
| **1min AI** | Quick AI platform |
| **Arli AI** | Creative AI models |

## Common Settings

All providers share these configuration options:

| Setting | Description |
|---------|-------------|
| **Model Name** | The specific model to use |
| **Temperature** | Creativity level (0-2) |
| **Credentials** | API key configuration |

### Temperature Settings

| Level | Value | Use Case |
|-------|-------|----------|
| None | 0 | Deterministic, factual responses |
| Low | 0.5 | Balanced, slightly creative |
| Medium | 1 | Default, good balance |
| High | 1.5 | More creative |
| Maximum | 2 | Most creative, experimental |

## Advanced Features

### Extended Thinking (Claude)

For Claude models with reasoning capabilities:
- **Disabled**: Standard response
- **Minimal**: 1,024 thinking tokens
- **Low**: 2,048 thinking tokens
- **Medium**: 5,120 thinking tokens
- **High**: 10,240 thinking tokens

### Reasoning Effort (OpenAI o-series)

For OpenAI reasoning models (o1, o3, o4):
- **Low**: Fast, economical
- **Medium**: Balanced
- **High**: More thorough reasoning

### Gemini Thinking

For Google Gemini 2.5+ models:
- **Disabled**: No thinking
- **Auto**: Model decides
- **Custom budgets**: 512 to 30,000 tokens

### Google Search Tool

Available for Gemini 2.5 models - grounds responses with real-time web content.

### URL Context Tool

Available for Gemini 2.5 models - allows including URLs for additional context.

## Choosing a Provider

<AccordionGroup>
  <Accordion title="Best for general use">
    - **OpenAI GPT-4o**: Excellent all-around performance
    - **Anthropic Claude 3.5 Sonnet**: Great for writing and analysis
    - **Google Gemini 2.5 Pro**: Long context, multimodal
  </Accordion>
  <Accordion title="Best for coding">
    - **Anthropic Claude 3.5 Sonnet**: Excellent code understanding
    - **DeepSeek Coder**: Specialized for code
    - **OpenAI GPT-4o**: Strong coding abilities
  </Accordion>
  <Accordion title="Best for speed">
    - **Groq**: Ultra-fast inference
    - **Cerebras**: Hardware-accelerated
    - **Sambanova**: High throughput
  </Accordion>
  <Accordion title="Best for privacy">
    - **Ollama**: Completely local
    - **LM Studio**: Local with GUI
  </Accordion>
  <Accordion title="Best for budget">
    - **EnConvo Cloud**: Pay with points
    - **DeepSeek**: Very affordable
    - **Groq Free Tier**: Free usage available
  </Accordion>
</AccordionGroup>

