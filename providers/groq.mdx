---
title: "Groq"
description: "Ultra-fast inference with Groq in EnConvo"
---

## Overview

Groq provides extremely fast inference using custom LPU hardware. Perfect for real-time applications requiring instant responses.

## Supported Models

| Model | Description | Speed |
|-------|-------------|-------|
| **Gemma2 9B IT** | Google's model | ~700 tok/s |
| **Llama 3.1 70B** | Meta's large model | ~300 tok/s |
| **Llama 3.1 8B** | Meta's small model | ~750 tok/s |
| **Mixtral 8x7B** | Mistral MoE | ~500 tok/s |

## Setup

<Steps>
  <Step title="Get API Key">
    1. Go to [Groq Console](https://console.groq.com)
    2. Sign in or create an account
    3. Navigate to **API Keys**
    4. Create a new API key
  </Step>
  <Step title="Configure in EnConvo">
    1. Open **Settings** â†’ **AI Provider**
    2. Select **Groq AI**
    3. Go to **Credentials** module
    4. Enter your API key
  </Step>
  <Step title="Select Model">
    Choose your preferred model from the dropdown
  </Step>
</Steps>

## Configuration

| Setting | Description | Default |
|---------|-------------|---------|
| **Credentials** | API key configuration | Required |
| **Model Name** | Model to use | Gemma2 9B IT |
| **Temperature** | Creativity (0-2) | Medium (1) |

## Reasoning Effort

For reasoning-capable models (GPT-OSS):

| Level | Description |
|-------|-------------|
| **Low** | Fast reasoning |
| **Medium** | Balanced |
| **High** | Thorough |

## Pricing

Groq offers generous free tiers. Check [Groq](https://groq.com) for current pricing.

<Note>
Groq's free tier is great for trying ultra-fast inference!
</Note>

## Why Groq?

<CardGroup cols={2}>
  <Card title="Speed" icon="bolt">
    Fastest inference available - 300-750+ tokens/second
  </Card>
  <Card title="Free Tier" icon="gift">
    Generous free usage for development
  </Card>
  <Card title="Quality Models" icon="star">
    Access to Llama, Gemma, Mixtral
  </Card>
  <Card title="Low Latency" icon="clock">
    Near-instant responses
  </Card>
</CardGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="When to use Groq">
    - Real-time chat applications
    - Quick iterations during development
    - Time-sensitive tasks
  </Accordion>
  <Accordion title="Model Selection">
    - **Llama 3.1 70B**: Best quality
    - **Llama 3.1 8B**: Fastest
    - **Gemma2 9B**: Good balance
  </Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Rate limits">
    - Free tier has rate limits
    - Wait and retry, or upgrade
  </Accordion>
</AccordionGroup>
